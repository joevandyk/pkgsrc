$NetBSD: patch-ac,v 1.2 2004/07/11 17:23:15 kristerw Exp $

--- xw_disp.c.orig	2000-03-22 17:01:00.000000000 +0100
+++ xw_disp.c	2004-07-11 19:10:44.000000000 +0200
@@ -24,11 +24,14 @@
 #include <X11/Xatom.h>
 #include <X11/keysym.h>
 #include <string.h>
+#if defined(__NetBSD__)
+#include <errno.h>
+#endif
 #include <sys/time.h>
 #include <sys/errno.h>
 /**/
 #include <X11/extensions/XShm.h>
-#ifdef __FreeBSD__
+#if defined(__FreeBSD__) || defined(__NetBSD__)
 #include <machine/param.h>
 #endif
 #include <sys/types.h>
@@ -79,36 +82,36 @@
 		case 15:
 		case 16:
 			for( x = 0 ; x < XSIZE; x+=8 ) {
-				__asm__ ("
-		pushl %0
-		pushl %2
-		popl %%edi
-		popl %%esi
-		movl %1,%%ebx
-
-		movzbl (%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,(%%edi)
-		movzbl 1(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,2(%%edi)
-		movzbl 2(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,4(%%edi)
-		movzbl 3(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,6(%%edi)
-		movzbl 4(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,8(%%edi)
-		movzbl 5(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,10(%%edi)
-		movzbl 6(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,12(%%edi)
-		movzbl 7(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
+				__asm__ ("\n\
+		pushl %0\n\
+		pushl %2\n\
+		popl %%edi\n\
+		popl %%esi\n\
+		movl %1,%%ebx\n\
+\n\
+		movzbl (%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,(%%edi)\n\
+		movzbl 1(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,2(%%edi)\n\
+		movzbl 2(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,4(%%edi)\n\
+		movzbl 3(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,6(%%edi)\n\
+		movzbl 4(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,8(%%edi)\n\
+		movzbl 5(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,10(%%edi)\n\
+		movzbl 6(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,12(%%edi)\n\
+		movzbl 7(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
 		movw %%ax,14(%%edi)" : : 
 		"g"(&(imgval[addr])),	/* %0 <= &imgval[] */
 		"g"(&palette),		/* %1 <= palette */
@@ -119,36 +122,36 @@
 			break;
 		case 8:
 			for( x = 0 ; x < XSIZE; x+=8 ) {
-				__asm__ ("
-		pushl %0
-		pushl %2
-		popl %%edi
-		popl %%esi
-		movl %1,%%ebx
-
-		movzbl (%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,(%%edi)
-		movzbl 1(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,1(%%edi)
-		movzbl 2(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,2(%%edi)
-		movzbl 3(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,3(%%edi)
-		movzbl 4(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,4(%%edi)
-		movzbl 5(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,5(%%edi)
-		movzbl 6(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%ax,6(%%edi)
-		movzbl 7(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
+				__asm__ ("\n\
+		pushl %0\n\
+		pushl %2\n\
+		popl %%edi\n\
+		popl %%esi\n\
+		movl %1,%%ebx\n\
+\n\
+		movzbl (%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,(%%edi)\n\
+		movzbl 1(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,1(%%edi)\n\
+		movzbl 2(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,2(%%edi)\n\
+		movzbl 3(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,3(%%edi)\n\
+		movzbl 4(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,4(%%edi)\n\
+		movzbl 5(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,5(%%edi)\n\
+		movzbl 6(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%ax,6(%%edi)\n\
+		movzbl 7(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
 		movb %%ax,7(%%edi)" : : 
 		"g"(&(imgval[addr])),	/* %0 <= &imgval[] */
 		"g"(&palette),		/* %1 <= palette */
@@ -205,77 +208,77 @@
 	/* update imgval, based on vram_c */
 	{
 #ifdef __i386__
-		__asm__ __volatile__ ("
-		movl %0,%%esi
-		movl %1,%%edi
-		
-		movb 0x8000(%%esi),%%ah
-		movb 0x4000(%%esi),%%bl
-		movb (%%esi),%%bh
-		
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,1(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,2(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,3(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,4(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,5(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
-		movb %%al,6(%%edi)
-		xorb %%al,%%al
-		shlb $1,%%ah
-		rclb $1,%%al
-		shlb $1,%%bl
-		rclb $1,%%al
-		shlb $1,%%bh
-		rclb $1,%%al
+		__asm__ __volatile__ ("\n\
+		movl %0,%%esi\n\
+		movl %1,%%edi\n\
+		\n\
+		movb 0x8000(%%esi),%%ah\n\
+		movb 0x4000(%%esi),%%bl\n\
+		movb (%%esi),%%bh\n\
+		\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,1(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,2(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,3(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,4(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,5(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
+		movb %%al,6(%%edi)\n\
+		xorb %%al,%%al\n\
+		shlb $1,%%ah\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bl\n\
+		rclb $1,%%al\n\
+		shlb $1,%%bh\n\
+		rclb $1,%%al\n\
 		movb %%al,7(%%edi)" : : 
 		"g"(&(vram_c[addr])),		/* %0 <= &vram_c[] */
 		"g"(&(imgval[addr*8]))		/* %1 <= &imgval[] */
@@ -302,34 +305,34 @@
 #ifdef __i386__
 		case 15:
 		case 16:
-			__asm__ ("
-		movl %2,%%edi
-		movl %0,%%esi
-		movl %1,%%ebx
-		
-		movzbl (%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,(%%edi)
-		movzbl 1(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,2(%%edi)
-		movzbl 2(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,4(%%edi)
-		movzbl 3(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,6(%%edi)
-		movzbl 4(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,8(%%edi)
-		movzbl 5(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,10(%%edi)
-		movzbl 6(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movw %%ax,12(%%edi)
-		movzbl 7(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
+			__asm__ ("\n\
+		movl %2,%%edi\n\
+		movl %0,%%esi\n\
+		movl %1,%%ebx\n\
+		\n\
+		movzbl (%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,(%%edi)\n\
+		movzbl 1(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,2(%%edi)\n\
+		movzbl 2(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,4(%%edi)\n\
+		movzbl 3(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,6(%%edi)\n\
+		movzbl 4(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,8(%%edi)\n\
+		movzbl 5(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,10(%%edi)\n\
+		movzbl 6(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movw %%ax,12(%%edi)\n\
+		movzbl 7(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
 		movw %%ax,14(%%edi)" : : 
 		"g"(&(imgval[bufaddr])),/* %0 <= &imgval[] */
 		"g"(&palette),		/* %1 <= palette */
@@ -337,34 +340,34 @@
 		: "ax","bx","si","di" );
 			break;
 		case 8:
-			__asm__ ("
-		movl %2,%%edi
-		movl %0,%%esi
-		movl %1,%%ebx
-		
-		movzbl (%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,(%%edi)
-		movzbl 1(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,1(%%edi)
-		movzbl 2(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,2(%%edi)
-		movzbl 3(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,3(%%edi)
-		movzbl 4(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,4(%%edi)
-		movzbl 5(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,5(%%edi)
-		movzbl 6(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
-		movb %%al,6(%%edi)
-		movzbl 7(%%esi),%%eax
-		movl (%%ebx,%%eax,4),%%eax
+			__asm__ ("\n\
+		movl %2,%%edi\n\
+		movl %0,%%esi\n\
+		movl %1,%%ebx\n\
+		\n\
+		movzbl (%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,(%%edi)\n\
+		movzbl 1(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,1(%%edi)\n\
+		movzbl 2(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,2(%%edi)\n\
+		movzbl 3(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,3(%%edi)\n\
+		movzbl 4(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,4(%%edi)\n\
+		movzbl 5(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,5(%%edi)\n\
+		movzbl 6(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
+		movb %%al,6(%%edi)\n\
+		movzbl 7(%%esi),%%eax\n\
+		movl (%%ebx,%%eax,4),%%eax\n\
 		movb %%al,7(%%edi)" : : 
 		"g"(&(imgval[bufaddr])),/* %0 <= &imgval[] */
 		"g"(&palette),		/* %1 <= palette */
